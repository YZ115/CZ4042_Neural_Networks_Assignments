{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, layers\n",
    "from tensorflow.python.client import device_lib\n",
    "import csv\n",
    "import re\n",
    "import pylab\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default variables\n",
    "MAX_DOCUMENT_LENGTH = 100\n",
    "N_FILTERS = 10\n",
    "FILTER_SHAPE1 = [20, 256]\n",
    "FILTER_SHAPE2 = [20, 1]\n",
    "POOLING_WINDOW = 4\n",
    "POOLING_STRIDE = 2\n",
    "MAX_LABEL = 15\n",
    "\n",
    "batch_size = 128\n",
    "no_epochs = 250\n",
    "lr = 0.01\n",
    "# Dropout with probability of 0.5\n",
    "use_dropout = False\n",
    "\n",
    "seed = 10\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is required when using GPU\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(gpus)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        devices = device_lib.list_local_devices()\n",
    "        avail_gpu = [x for x in devices if x.device_type == 'GPU']\n",
    "        print('\\n',avail_gpu)\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No gpus available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data with [character]\n",
    "def vocabulary(strings):\n",
    "    chars = sorted(list(set(list(''.join(strings)))))\n",
    "    char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "    vocab_size = len(chars)\n",
    "    return vocab_size, char_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(strings, char_to_ix, MAX_LENGTH):\n",
    "    data_chars = [list(d.lower()) for _, d in enumerate(strings)]\n",
    "    for i, d in enumerate(data_chars):\n",
    "        if len(d)>MAX_LENGTH:\n",
    "            d = d[:MAX_LENGTH]\n",
    "        elif len(d) < MAX_LENGTH:\n",
    "            d += [' '] * (MAX_LENGTH - len(d))\n",
    "            \n",
    "    data_ids = np.zeros([len(data_chars), MAX_LENGTH], dtype=np.int64)\n",
    "    for i in range(len(data_chars)):\n",
    "        for j in range(MAX_LENGTH):\n",
    "            data_ids[i, j] = char_to_ix[data_chars[i][j]]\n",
    "    return np.array(data_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_chars():\n",
    "    x_train, y_train, x_test, y_test = [], [], [], []\n",
    "    cop = re.compile(\"[^a-z^A-Z^0-9^,^.^' ']\")\n",
    "    with open('./data/train_medium.csv', encoding='utf-8') as filex:\n",
    "        reader = csv.reader(filex)\n",
    "        for row in reader:\n",
    "            data = cop.sub(\"\", row[1])\n",
    "            x_train.append(data)\n",
    "            y_train.append(int(row[0]))\n",
    "\n",
    "    with open('./data/test_medium.csv', encoding='utf-8') as filex:\n",
    "        reader = csv.reader(filex)\n",
    "        for row in reader:\n",
    "            data = cop.sub(\"\", row[1])\n",
    "            x_test.append(data)\n",
    "            y_test.append(int(row[0]))\n",
    "    vocab_size, char_to_ix = vocabulary(x_train+x_test)\n",
    "    x_train = preprocess(x_train, char_to_ix, MAX_DOCUMENT_LENGTH)\n",
    "    y_train = np.array(y_train)\n",
    "    x_test = preprocess(x_test, char_to_ix, MAX_DOCUMENT_LENGTH)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    x_train = tf.constant(x_train, dtype=tf.int64)\n",
    "    y_train = tf.constant(y_train, dtype=tf.int64)\n",
    "    x_test = tf.constant(x_test, dtype=tf.int64)\n",
    "    y_test = tf.constant(y_test, dtype=tf.int64)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_history(model_name, dropout):\n",
    "    if dropout:\n",
    "        with open(f'./histories/{model_name}_dropout') as file:\n",
    "            history = json.load(file)\n",
    "    else:\n",
    "        with open(f'./histories/{model_name}_nodropout') as file:\n",
    "            history = json.load(file)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_step(model, x, label, drop_rate):\n",
    "    with tf.GradientTape() as tape:\n",
    "        out = model(x, drop_rate)\n",
    "        loss = loss_object(label, out)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing function\n",
    "def test_step(model, x, label, drop_rate=0):\n",
    "    out = model(x,drop_rate)\n",
    "    t_loss = loss_object(label, out)\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(label, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float32')\n",
    "class CharCNN(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, drop_rate):\n",
    "        super(CharCNN, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.drop_rate = drop_rate\n",
    "        \n",
    "        # Weight variables and RNN cell\n",
    "        self.conv1 = tf.keras.layers.Conv2D(N_FILTERS, FILTER_SHAPE1, padding='VALID', activation='relu', use_bias=True)\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D(POOLING_WINDOW, POOLING_STRIDE, padding='SAME')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(N_FILTERS, FILTER_SHAPE2, padding='VALID', activation='relu', use_bias=True)\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D(POOLING_WINDOW, POOLING_STRIDE, padding='SAME')\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense = tf.keras.layers.Dense(MAX_LABEL, activation='softmax')\n",
    "\n",
    "    def call(self, x, drop_rate):\n",
    "        # forward\n",
    "        x = tf.one_hot(x, 256)\n",
    "        x = x[..., tf.newaxis] \n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)    \n",
    "        x = self.flatten(x)\n",
    "        x = tf.nn.dropout(x, drop_rate)\n",
    "        logits = self.dense(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = read_data_chars()\n",
    "# Use `tf.data` to batch and shuffle the dataset:\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)).shuffle(10000).batch(batch_size)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
    "\n",
    "# Build model\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "test_acc = []\n",
    "train_acc = []\n",
    "test_l = []\n",
    "train_l = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'CharCNN'\n",
    "\n",
    "if use_dropout:\n",
    "    model = CharCNN(256,0.5)\n",
    "    print(\"Using dropout, rate=0.5\")\n",
    "else:\n",
    "    model = CharCNN(256,0)\n",
    "\n",
    "# Choose optimizer and loss function for training\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "# Select metrics to measure the loss and the accuracy of the model. \n",
    "# These metrics accumulate the values over epochs and then print the overall result.\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "for epoch in range(no_epochs):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "    \n",
    "    if use_dropout:\n",
    "        for images, labels in train_ds:\n",
    "            train_step(model, images, labels, drop_rate=0.5)\n",
    "    else:\n",
    "        for images, labels in train_ds:\n",
    "            train_step(model, images, labels, drop_rate=0)\n",
    "\n",
    "    for images, labels in test_ds:\n",
    "        test_step(model, images, labels, drop_rate=0)\n",
    "\n",
    "    test_acc.append(test_accuracy.result().numpy().astype(float))\n",
    "    train_acc.append(train_accuracy.result().numpy().astype(float))\n",
    "    test_l.append(test_loss.result().numpy().astype(float))\n",
    "    train_l.append(train_loss.result().numpy().astype(float))\n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "    print (template.format(epoch+1,\n",
    "                          train_loss.result(),\n",
    "                          train_accuracy.result(),\n",
    "                          test_loss.result(),\n",
    "                          test_accuracy.result()))\n",
    "end_time = time.perf_counter()\n",
    "time_taken = end_time-start_time\n",
    "print(\"Time taken for training: \"+str(time_taken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Time taken for training: \"+str(time_taken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {name: {\n",
    "    'accuracy': train_acc,\n",
    "    'val_acc': test_acc,\n",
    "    'loss': train_l,\n",
    "    'val_loss': test_l,\n",
    "    'time_taken': time_taken\n",
    "}}\n",
    "\n",
    "if use_dropout:\n",
    "    with open(f'./histories/{name}_dropout', 'w') as file:\n",
    "        json.dump(history, file)\n",
    "else:\n",
    "    with open(f'./histories/{name}_nodropout', 'w') as file:\n",
    "        json.dump(history, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for keys in history[name]:\n",
    "    print(keys)\n",
    "    print(history[name][keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history = load_history(name,True)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(range(1, len(history['CharCNN']['accuracy']) + 1), \n",
    "         history['CharCNN']['accuracy'], label='Train Accuracy')\n",
    "plt.plot(range(1, len(history['CharCNN']['accuracy']) + 1), \n",
    "         history['CharCNN']['val_acc'], label='Test Accuracy')\n",
    "plt.title('CharCNN Model Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.savefig(\n",
    "            f'./results/QuestionB1_CharCNN_accuracy.pdf'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(range(1, len(history['CharCNN']['loss']) + 1), \n",
    "         history['CharCNN']['loss'], label='Train Loss')\n",
    "plt.plot(range(1, len(history['CharCNN']['loss']) + 1), \n",
    "         history['CharCNN']['val_loss'], label='Test Loss')\n",
    "plt.title('CharCNN Model Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.savefig(\n",
    "            f'./results/QuestionB1_CharCNN_loss.pdf'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
